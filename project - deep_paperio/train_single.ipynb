{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"game/\")\n",
    "# from paperio2_wrapper import *\n",
    "import player\n",
    "from paperio_single_wrapper import PaperIOSingleWrapper\n",
    "from paperio_two_wrapper import PaperIOTwoWrapper\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import random\n",
    "from collections import deque\n",
    "import warnings\n",
    "from layer_utils import *\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 4  # number of valid actions\n",
    "GAMMA = 0.99  # decay rate of past observations\n",
    "OBSERVE = 10000.  # timesteps to observe before training\n",
    "EXPLORE = 20000.  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.08  # final value of epsilon\n",
    "INITIAL_EPSILON = 0.2  # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000  # number of previous transitions to remember\n",
    "BATCH = 64  # size of minibatch\n",
    "FRAME_PER_ACTION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_single():\n",
    "    s = tf.placeholder(\"float\", [None, 15, 15, 8])\n",
    "    # 15 : view size\n",
    "\n",
    "    x = conv2d(s, 32, 3, 1)\n",
    "    x = maxpool(x)\n",
    "    x = conv2d(x, 64, 3, 1)\n",
    "    x = tf.layers.dropout(x)\n",
    "    x = conv2d(x, 128, 3, 1)\n",
    "\n",
    "    x = flatten(x)\n",
    "\n",
    "    x = fully_conn(x, 256)\n",
    "    x = fully_conn(x, 64)\n",
    "    out = output(x, ACTIONS)\n",
    "    return s, out, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_action(direction):\n",
    "    action = np.zeros([ACTIONS])\n",
    "    action[direction] = 1\n",
    "    return action\n",
    "\n",
    "\n",
    "def initial_state(areas, paths):\n",
    "    return np.stack((areas, paths, areas, paths, areas, paths, areas, paths), axis=2)\n",
    "\n",
    "\n",
    "def update_state(areas, paths, state):\n",
    "    return np.append(np.stack((areas, paths), axis=2), state[:, :, :6], axis=2)\n",
    "\n",
    "\n",
    "def to_action_dict(action_t):\n",
    "    d = {}\n",
    "    for k, v in action_t.items():\n",
    "        d[k] = np.argmax(v)\n",
    "    return d\n",
    "\n",
    "def get_heads(N):\n",
    "    N_4 = N // 4\n",
    "    # If you want to have randome head, let h1 = h2 = None\n",
    "    return None, None\n",
    "    # else: there are some options:\n",
    "    \n",
    "    # center: used in single player\n",
    "    # return (N // 2, N // 2), (N // 2, N // 2)\n",
    "    \n",
    "    # 1/4 corner:\n",
    "    # return (N_4, N_4), (N_4 * 3, N_4 * 3)\n",
    "    \n",
    "    # random corner:\n",
    "    # if random.random() > 0.5:\n",
    "    #     h1, h2 = (N_4, N_4 * 3), (N_4 * 3, N_4)\n",
    "    # else:\n",
    "    #     h1, h2 = (N_4, N_4), (N_4 * 3, N_4 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(s, readout, sess, N=50, MAX_ITER=None, \n",
    "              is_two_player=True, is_print=False, has_random=True, only_observe=False):\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost, global_step=global_step)\n",
    "\n",
    "    DEAD_REWARD = -N * N\n",
    "    P1 = 1\n",
    "    P2 = 2\n",
    "    PLAYERS = {P1, P2} if is_two_player else {P1}\n",
    "    N_4 = N // 4\n",
    "    \n",
    "    h1, h2 = get_heads(N)\n",
    "    \n",
    "    game_wrapper = PaperIOTwoWrapper(N=N, \n",
    "                                     id1=P1, \n",
    "                                     id2=P2, \n",
    "                                     h1=h1,\n",
    "                                     h2=h2,\n",
    "                                     init_pad=3, \n",
    "                                     is_two_player=is_two_player)\n",
    "    MAX_STEP = N * N / 4\n",
    "    prev_reset = 0\n",
    "    action_t = {}\n",
    "    init_states = {p: np.stack([np.zeros((15, 15))] * 8, axis=2) for p in PLAYERS}\n",
    "    D = deque()\n",
    "\n",
    "    for i in range(5):\n",
    "        action_t = {p: set_action(player.get_rand_dir()) for p in PLAYERS}\n",
    "        game_wrapper.step(to_action_dict(action_t))\n",
    "        for p in game_wrapper.players:\n",
    "            area, path = game_wrapper.get_views(p)\n",
    "            init_states[p] = update_state(area, path, init_states[p])\n",
    "\n",
    "    state_t = init_states\n",
    "\n",
    "    #     saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks_2\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    epsilon = INITIAL_EPSILON if has_random else 0\n",
    "    t = 0\n",
    "    while MAX_ITER is None or t < MAX_ITER:\n",
    "        for p in PLAYERS:\n",
    "            readout_t = readout.eval(feed_dict={s: [state_t[p]]})[0]\n",
    "            action = np.zeros([ACTIONS])\n",
    "            action_index = np.argmax(readout_t) if random.random() > epsilon else random.randrange(ACTIONS)\n",
    "            action[action_index] = 1\n",
    "            action_t[p] = action\n",
    "\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        game_wrapper.step(to_action_dict(action_t))\n",
    "\n",
    "        state_t1 = {}\n",
    "        \n",
    "        for p in PLAYERS:\n",
    "            area, path = game_wrapper.get_views(p)\n",
    "            state_t1[p] = update_state(area, path, state_t[p])\n",
    "\n",
    "        need_reset = False\n",
    "        \n",
    "        for p in PLAYERS:\n",
    "            is_dead = game_wrapper.is_dead(p)\n",
    "            is_win = game_wrapper.game.board.current_areas.get(p, 0) / (N * N) > 0.7\n",
    "            r0 = r1 = 0\n",
    "\n",
    "            if is_dead:\n",
    "                r0 = DEAD_REWARD\n",
    "            elif is_two_player and game_wrapper.is_dead(3 - p) and (3 - p) not in game_wrapper.game.board.suicide:\n",
    "                r0 = game_wrapper.prev_area[3 - p]\n",
    "            elif game_wrapper.is_in_area(p):\n",
    "                r0 = game_wrapper.get_delta_area(p)\n",
    "                r1 = 2 * game_wrapper.delta_dis(p)\n",
    "            else:\n",
    "                r0 = -0.7\n",
    "                r1 = -2 * game_wrapper.delta_dis(p)\n",
    "            reward_t = r0 + r1\n",
    "\n",
    "            if is_dead or is_win or t - prev_reset >= MAX_STEP:\n",
    "                need_reset = True\n",
    "                \n",
    "            D.append((state_t[p],\n",
    "                      action_t[p],\n",
    "                      reward_t,\n",
    "                      state_t1[p],\n",
    "                      is_dead or is_win))\n",
    "\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "            if t > OBSERVE:\n",
    "                # sample a minibatch to train on\n",
    "                minibatch = random.sample(D, BATCH)\n",
    "\n",
    "                # get the batch variables\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                r_batch = [d[2] for d in minibatch]\n",
    "                s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "                y_batch = []\n",
    "                readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})\n",
    "                for i in range(0, len(minibatch)):\n",
    "                    terminal = minibatch[i][4]\n",
    "                    # if terminal, only equals reward\n",
    "                    if terminal:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "                # perform gradient step\n",
    "                train_step.run(feed_dict={\n",
    "                    y: y_batch,\n",
    "                    a: a_batch,\n",
    "                    s: s_j_batch}\n",
    "                )\n",
    "\n",
    "            if t % 5000 == 0 and not only_observe:\n",
    "                saver.save(sess, 'saved_networks_2/dqn', global_step=global_step)\n",
    "                game_wrapper.game.show_board()\n",
    "\n",
    "            if is_print and t % 10 == 0:\n",
    "                # if is_print:\n",
    "                game_wrapper.game.show_board()\n",
    "\n",
    "            if is_print or t % 1000 == 0:\n",
    "                # print info\n",
    "                state = \"\"\n",
    "                if t <= OBSERVE:\n",
    "                    state = \"observe\"\n",
    "                elif OBSERVE < t <= OBSERVE + EXPLORE:\n",
    "                    state = \"explore\"\n",
    "                else:\n",
    "                    state = \"train\"\n",
    "\n",
    "                print(\"T:\", t, \"/ S:\", state, \"/ EPS:\", epsilon, \"/ P:\", p, \"/ ACT\", player.DIR_STR[np.argmax(action_t[p])],\n",
    "                      \"/ R0\", r0, \"/ R1\", r1, \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "        if need_reset:\n",
    "            # We want to reset it with different initial area\n",
    "            h1, h2 = get_heads(N)\n",
    "            game_wrapper.reset(h1=h1, h2=h2,\n",
    "                               init_pad=np.random.randint(7 - 3, 7))\n",
    "            prev_reset = t\n",
    "        state_t = state_t1\n",
    "        t += 1\n",
    "    saver.save(sess, 'saved_networks_2/dqn', global_step=global_step)\n",
    "    game_wrapper.game.show_board()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "s, readout, h_fc1 = create_network_single()\n",
    "SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Let's start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is for taking a look without change anything\n",
    "\n",
    "# train_single_network(s, \n",
    "#                      readout, \n",
    "#                      sess, \n",
    "#                      N=SIZE, \n",
    "#                      is_print=True,# if True, print image and every time stamp, otherwise no image \n",
    "#                      has_random=False, # if False, epsilon will always be zero, only model decide next move\n",
    "#                      only_observe=True # if True, model will not save in file, program stop afer OBSERVE\n",
    "#                     )\n",
    "\n",
    "# ===== This is for actual training ===== \n",
    "\n",
    "# train_single_network(s, readout, sess, N=SIZE, is_print=False, has_random=True, only_observe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_network(s, \n",
    "              readout, \n",
    "              sess, \n",
    "              N=SIZE, \n",
    "              is_two_player=False, # if False, then it will run in single player\n",
    "              is_print=True,# if True, print image and every time stamp, otherwise no image \n",
    "              has_random=False, # if False, epsilon will always be zero, only model decide next move\n",
    "              only_observe=True # if True, model will not save in file, program stop afer OBSERVE\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
