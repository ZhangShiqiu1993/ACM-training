{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "from paperio2_wrapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 4 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 10000. # timesteps to observe before training\n",
    "EXPLORE = 20000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.08 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.2 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 64 # size of minibatch\n",
    "FRAME_PER_ACTION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize=3, conv_stride = 1):\n",
    "    _, input_width, input_height, input_depth = x_tensor.get_shape().as_list()\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize, conv_ksize, input_depth, conv_num_outputs], \n",
    "                                              mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    biases = tf.Variable(tf.zeros(conv_num_outputs), dtype=tf.float32)\n",
    "\n",
    "    conv = tf.nn.conv2d(input=x_tensor, filter=weights, strides=[1, conv_stride, conv_stride, 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, biases)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "def maxpool(x_tensor):\n",
    "    return tf.nn.max_pool(x_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs = x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    # input layer\n",
    "    s = tf.placeholder(\"float\", [None, 50, 50, 8])\n",
    "    \n",
    "    x = conv2d(s, 64, 5, 2)\n",
    "    x = maxpool(x)\n",
    "    x = conv2d(x, 128, 3, 1)\n",
    "    x = tf.layers.dropout(x)    \n",
    "    x = conv2d(x, 128, 3, 1)\n",
    "    \n",
    "    x = flatten(x) \n",
    "    \n",
    "    x = fully_conn(x, 256)\n",
    "    x = fully_conn(x, 64)\n",
    "    out = output(x, ACTIONS)\n",
    "    return s, out, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game.player import UP, DOWN, LEFT, RIGHT, get_opposite_dir\n",
    "\n",
    "# if you want to print image\n",
    "# EDIT HERE!!!!!!!\n",
    "PRINT = False\n",
    "\n",
    "def set_action(direction):\n",
    "    action = np.zeros([ACTIONS])\n",
    "    action[direction] = 1\n",
    "    return action\n",
    "\n",
    "def totalReward(step, area, ter) :\n",
    "    return step + area + ter\n",
    "\n",
    "def isTermianl(terRew, ter) :\n",
    "    return terRew == ter or ter + terRew == 0\n",
    "\n",
    "def initial_state(areas, paths) :\n",
    "    return np.stack((areas, paths, areas, paths, areas, paths, areas, paths), axis=2)\n",
    "\n",
    "def update_state(areas, paths, state):\n",
    "    return np.append(np.stack((areas, paths), axis=2), state[:, :, :6], axis=2)\n",
    "\n",
    "def to_action_dict(action_t):\n",
    "    d = {}\n",
    "    for k, v in action_t.items():\n",
    "        d[k] = np.argmax(v)\n",
    "    return d\n",
    "\n",
    "def trainNetwork(s, readout, h_fc1, sess):\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    terminal_reward = 50\n",
    "    step_penalty = -0.7\n",
    "    game = PaperIO2Wrapper(50, step_penalty, terminal_reward)\n",
    "    PLAYER1 = game.p1.id\n",
    "    PLAYER2 = game.p2.id\n",
    "    D = deque()\n",
    "\n",
    "    action_t = {PLAYER1: set_action(DOWN), PLAYER2: set_action(UP)}\n",
    "\n",
    "    areas, paths, step_penalty, delta_area, termination_rewards = game.frame_step(to_action_dict(action_t))\n",
    "    revArea, revPath = game.get_player2_view()\n",
    "    state_t = {\n",
    "        PLAYER1: initial_state(areas, paths),\n",
    "        PLAYER2: initial_state(revArea, revPath)\n",
    "    }\n",
    "\n",
    "\n",
    "#     saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    while True:\n",
    "        for player in [PLAYER1, PLAYER2]:\n",
    "            readout_t = readout.eval(feed_dict={s : [state_t[player]]})[0]\n",
    "            action = np.zeros([ACTIONS])\n",
    "            action_index = np.argmax(readout_t) if random.random() > epsilon else random.randrange(ACTIONS)\n",
    "            if player == PLAYER2:\n",
    "                action_index = get_opposite_dir(action_index)\n",
    "            action[action_index] = 1\n",
    "            action_t[player] = action\n",
    "\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        \n",
    "        areas, paths, step_penalty, delta_area, termination_rewards = game.frame_step(to_action_dict(action_t))\n",
    "        revArea, revPath = game.get_player2_view()\n",
    "        \n",
    "        state_t1 ={\n",
    "            PLAYER1: update_state(areas, paths, state_t[PLAYER1]),\n",
    "            PLAYER2: update_state(revArea, revPath, state_t[PLAYER2])\n",
    "        } \n",
    "                \n",
    "        for player in [PLAYER1, PLAYER2]:\n",
    "            reward_t = totalReward(step_penalty, delta_area[player], termination_rewards[player])\n",
    "            D.append((state_t[player],\n",
    "                     action_t[player],\n",
    "                     reward_t,\n",
    "                     state_t1[player],\n",
    "                     isTermianl(termination_rewards[player], terminal_reward)))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "            # only train if done observing\n",
    "            if t > OBSERVE:\n",
    "                # sample a minibatch to train on\n",
    "                minibatch = random.sample(D, BATCH)\n",
    "\n",
    "                # get the batch variables\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                r_batch = [d[2] for d in minibatch]\n",
    "                s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "                y_batch = []\n",
    "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "                for i in range(0, len(minibatch)):\n",
    "                    terminal = minibatch[i][4]\n",
    "                    # if terminal, only equals reward\n",
    "                    if terminal:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "                # perform gradient step\n",
    "                train_step.run(feed_dict = {\n",
    "                    y : y_batch,\n",
    "                    a : a_batch,\n",
    "                    s : s_j_batch}\n",
    "                )\n",
    "\n",
    "            # update the old values\n",
    "            state_t = state_t1\n",
    "            t += 1\n",
    "\n",
    "            # save progress every 5000 iterations\n",
    "            if t % 5000 == 0:\n",
    "                saver.save(sess, 'saved_networks/dqn', global_step = t)\n",
    "\n",
    "            if PRINT and t % 10 == 0:\n",
    "                game.show_board()\n",
    "                \n",
    "            if PRINT or t % 2000 == 0:\n",
    "                # print info\n",
    "                state = \"\"\n",
    "                if t <= OBSERVE:\n",
    "                    state = \"observe\"\n",
    "                elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "                    state = \"explore\"\n",
    "                else:\n",
    "                    state = \"train\"\n",
    "\n",
    "                print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "                    \"/ EPSILON\", epsilon, \"/ ACTION\", np.argmax(action_t[player]), \"/ REWARD\", reward_t, \\\n",
    "                    \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "s, readout, h_fc1 = createNetwork()\n",
    "trainNetwork(s, readout, h_fc1, sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
